# -*- coding: utf-8 -*-
"""ChatBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6AuBBHUJrHnYfjxHTqqAcUIVh2O7ADe

## Save & Download Environments Requirements
"""

# 📦 Save current Python package environment to a requirements file
!pip freeze > requirements.txt

# 📥 Import files module from Google Colab to enable file download
from google.colab import files

# 💾 Download the generated requirements.txt file to local machine
files.download('requirements.txt')

#This cell saves the current Python environment's packages to requirements.txt and downloads the file locally for backup.

"""## Verify and Display Installed Package Versions"""

# 🔍 Check versions and details of key installed packages
!pip show gradio torch transformers sentencepiece deep-translator fuzzywuzzy python-Levenshtein huggingface_hub langdetect pandas numpy

# This helps verify that all required packages are installed and to know their exact versions.

# 📦 Install Core Deep Learning Libraries
# !pip install torch==2.4.0                   # PyTorch – Core deep learning framework for model inference & tensor ops
# !pip install transformers==4.44.2           # Hugging Face Transformers – Load & run pretrained NLP models
# !pip install sentencepiece==0.2.0           # Tokenizer for multilingual models (e.g., mT5, MarianMT)
# !pip install huggingface-hub==0.24.6        # Download models & datasets from Hugging Face Hub
# !pip install accelerate==0.33.0             # Optimize model inference/training on multiple devices

# 🖥️ Install Chatbot Web Interface Library
# !pip install gradio==5.41.0                 # User-friendly web UI for chatbot interaction & testing

# 🌐 Install Translation & Language Detection Tools
# !pip install deep-translator==1.11.4        # Translate queries & bot responses to multiple languages
# !pip install langdetect==1.0.9              # Detects language of user input for translation logic

# 🔍 Install String Matching & Fuzzy Search Libraries
# !pip install fuzzywuzzy==0.18.0             # Approximate string matching for better FAQ matching
# !pip install python-Levenshtein==0.25.1     # Speed up fuzzy matching using optimized Levenshtein distance

# 📊 Install Data Handling & Processing Libraries
# !pip install pandas==2.2.2                  # Handle CSV files & tabular data for FAQ dataset
# !pip install numpy==1.26.4                  # Numerical computations & array operations

# ⚙️ Install Utility Libraries
# !pip install tqdm==4.66.4                    # Progress bars for loops & processing steps
# !pip install python-dotenv==1.0.1            # Load API keys & environment variables securely

#This section checks installed package versions and lists the key libraries used in the project along with their purpose.

"""## Packages Installation"""

# Install Required Python Libraries for Data Cleaning, EDA, and Tokenizer Setup

# Install required Python libraries quietly (no verbose output)
!pip install pandas matplotlib seaborn langdetect --quiet    # For data handling, visualization, and language detection
!pip install transformers --upgrade --quiet                  # Hugging Face transformers for tokenization and NLP models

# 📦 Libraries Installed and Their Purpose
# 🐼 pandas               → For data loading, cleaning, and analysis of tabular data
# 📊 matplotlib & seaborn → For creating static and interactive data visualizations
# 🤗 transformers         → Hugging Face library for tokenization and transformer-based models (e.g., translation, classification)
# 🔤 langdetect           → Language detection library to identify input text language (optional for multilingual support)

# Summary
# - Installed essential libraries for data processing, visualization, and NLP
# - Used quiet flag (--quiet) to minimize installation logs in Colab

# Short Description:
# Setup of core Python libraries for data analysis, visualization, language detection, and transformer-based NLP models.

# Key Features:

# - Data cleaning and analysis with pandas
# - Visualizations using matplotlib and seaborn
# - Language detection capability with langdetect
# - Transformer tokenizer and model support via Hugging Face transformers

# Models/Tools Used:
# Hugging Face Transformers (for tokenization & NLP)
# langdetect (language identification)

# 📦 Commands / Functions Used:

# Python Package Management
# !pip install package_name --quiet → Installs Python packages quietly in the Colab environment

# pandas                      → For data manipulation and analysis of tabular datasets
# matplotlib & seaborn        → For plotting and visualization of data
# transformers (Hugging Face) → Provides pre-trained models and tokenizers for various NLP tasks
# langdetect                  → Detects language from text input and returns ISO language codes

"""## Data Cleaning"""

# Load & Preview Dataset

# 📥 Step 1: Import Required Libraries
import pandas as pd         # Data Handling
import re                   # Regex for text cleaning

# 🔗 Step 2: Load Dataset from Google Sheets CSV link
csv_link = "https://docs.google.com/spreadsheets/d/e/2PACX-1vSMF2sHzDY3eelpyYMwUIFKdfiGijHEADmRGzOsZP6Q_2htfhZ7KKh666LcYvUkiQ/pub?output=csv"
df = pd.read_csv(csv_link)

# 👀 Step 3: Preview the First 5 Rows
df.head()

# Summary
# - Loaded dataset from a public Google Sheets CSV link into a pandas DataFrame
# - Imported essential libraries for data handling and text cleaning
# - Previewed the first 5 rows for initial data inspection

# Short Description:
# This block loads the dataset directly from a Google Sheets CSV export link and previews its first few records for initial exploration.

# Key Features:
# - Data import from online source using pandas
# - Basic setup with regex library imported for possible text cleaning
# - Initial data preview with df.head()

# Models/Tools Used:
# - pandas for data loading and manipulation
# - re for regex-based text processing (imported, but not yet used)

# 📦 Commands / Functions Used:

# pandas
# pd.read_csv(url)         → Reads CSV data from the provided URL into a DataFrame
# df.head()                → Displays the first 5 rows of the DataFrame

# re
# import re                → Imports Python's regular expressions library for text processing (for later use)

# Clean Text Columns

# Define Text Cleaning Function (multilingual-safe)
def clean_text(text):
    text = str(text).strip().lower()                    # Lowercase & trim
    text = re.sub(r"[–—−]", "-", text)                  # Normalize different hyphens
    text = text.replace("’", "'")                       # Normalize apostrophes
    text = text.replace("“", '"').replace("”", '"')     # Normalize double quotes
    text = re.sub(r"<.*?>", "", text)                   # Remove HTML tags if any
    text = re.sub(r"\s+", " ", text)                    # Collapse extra spaces
    # Keep Unicode letters (for Tamil, Hindi, etc.) + numbers + punctuation
    text = re.sub(r"[^\w\s.,?!'’-]", '', text, flags=re.UNICODE)
    return text

# Apply Cleaning to User Query and BOT Response Columns
df['User Question'] = df['User Question'].apply(clean_text) # Apply clean_text function to each element in 'User Question' column
df['BOT Response'] = df['BOT Response'].apply(clean_text)   # Apply clean_text function to each element in 'BOT Response' column

# Preview Cleaned Data
df.head()

# Summary
# - Created a multilingual-safe text cleaning function for uniform preprocessing
# - Normalized hyphens, apostrophes, quotes, and removed HTML tags
# - Removed unwanted special characters, preserving Unicode letters and basic punctuation
# - Applied cleaning function to 'User Question' and 'BOT Response' columns
# - Previewed cleaned text data for validation

# Short Description:
# This block defines and applies a comprehensive text cleaning function to preprocess multilingual textual data, ensuring consistency and removing noise for better analysis or NLP tasks.

# Key Features:
# - Lowercasing and trimming of text
# - Normalization of punctuation and special characters
# - Removal of HTML tags and extra spaces
# - Preservation of Unicode characters for multilingual support
# - Applied to key text columns in dataset

# Models/Tools Used:
# - Python's re module for regex-based text processing

# 📦 Commands / Functions Used:

# Python & Regex Utilities
# str(text).strip().lower()       → Convert input to string, trim spaces, and lowercase text
# re.sub(pattern, replacement, text) → Regex substitution for pattern replacements
# .apply(function)                → Pandas method to apply a function element-wise on a Series

"""## Data Preprocessing"""

# Data Preprocessing - Handle missing values, remove duplicates, and ensure the dataset is clean for modeling.

# 📦 Import Required Library
import pandas as pd

# 👁️ Step 1: Quick Overview of the Dataset
df.info()                   # Show data types & non-null counts
df.describe(include='all')  # Get descriptive statistics

# 🔎 Step 2: Check for Missing (NaN) Values
df.isnull().sum()           # Sum of missing values in each column

# 🧼 Step 3: Drop Rows with Missing Values (if any)
# ⚠️ Optional: Only do this if nulls are few and not useful
df = df.dropna()

# Confirm removal
df.isnull().sum()           # Check missing values again to confirm removal

# 🧮 Step 4: Check for Duplicates
duplicates = df.duplicated()
print(f"🔁 Total Duplicates Found: {duplicates.sum()}")  # Print total duplicates count

# 🧽 Step 5: Remove Duplicate Rows (if any)
df = df.drop_duplicates()

# Confirm No More Duplicates
print(f"✔️ Shape after cleaning: {df.shape}")            # Print dataset shape after cleaning

# 🔄 Step 6: Rename Columns (if needed for consistency)
df.rename(columns={'User Question': 'user_input', 'BOT Response': 'bot_output'}, inplace=True)

# Confirm Column Names
df.columns                                             # Display updated column names

# Summary
# - Performed initial data exploration with info() and describe()
# - Checked for and dropped rows with missing values
# - Identified and removed duplicate rows
# - Renamed key columns for consistency and ease of use
# - Ensured dataset is clean and ready for further processing or modeling

# Short Description:
# This block handles core data preprocessing tasks: overview, missing value treatment, duplicate removal, and column renaming to prepare the dataset for modeling.

# Key Features:
# - Dataset information and statistics overview
# - Missing data detection and removal
# - Duplicate detection and removal
# - Column renaming for standardization

# Models/Tools Used:
# - pandas library for data handling and cleaning

# 📦 Commands / Functions Used:

# pandas
# df.info()                             → Provides summary of DataFrame including data types and non-null counts
# df.describe(include='all')            → Generates descriptive statistics for all columns
# df.isnull().sum()                     → Counts missing values in each column
# df.dropna()                           → Removes rows containing missing values
# df.duplicated()                       → Identifies duplicate rows returning a boolean Series
# df.drop_duplicates()                  → Removes duplicate rows from DataFrame
# df.rename(columns=dict, inplace=True) → Renames columns as per the provided dictionary in place
# df.columns                            → Lists column names of the DataFrame

"""## Exploratory Data Analysis (EDA)"""

# Exploratory Data Analysis (EDA)

# 📊 Goal: Understand the structure and distribution of text data before modeling.

# 📥 Import necessary libraries for data analysis and visualization
import pandas as pd                    # For data manipulation
import matplotlib.pyplot as plt        # For plotting
import seaborn as sns                  # For advanced plotting

# 📂 Load the cleaned dataset from CSV
df = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vSMF2sHzDY3eelpyYMwUIFKdfiGijHEADmRGzOsZP6Q_2htfhZ7KKh666LcYvUkiQ/pub?output=csv")

# 👀 Preview the first few rows of the dataset
df.head()    # Check structure: user_input and bot_output columns must exist

# Summary
# - Imported libraries for data manipulation and visualization
# - Loaded cleaned dataset from Google Sheets CSV link
# - Previewed initial rows to confirm data structure and key columns

# Short Description:
# This block performs initial exploratory data analysis by loading and previewing the cleaned dataset to verify its readiness for further analysis and modeling.

# Key Features:
# - Importing pandas, matplotlib, and seaborn for EDA
# - Loading dataset from an online CSV source
# - Previewing dataset structure and content

# Models/Tools Used:
# - pandas for data manipulation
# - matplotlib and seaborn for visualization (imports only, no plots generated here)

# 📦 Commands / Functions Used:

# pandas
# pd.read_csv(url)     → Reads CSV data from the provided URL into a DataFrame
# df.head()            → Displays the first 5 rows of the DataFrame

# matplotlib.pyplot
# import matplotlib.pyplot as plt  → Import plotting library (used later for visualization)

# seaborn
# import seaborn as sns            → Import statistical data visualization library (used later)

# Data Cleaning, Text Preprocessing & Length Analysis (EDA)

# Rename columns to lowercase snake_case
df.rename(columns={
    'User Question': 'user_input',
    'BOT Response': 'bot_output'
}, inplace=True)

# Drop rows with missing user_input or bot_output
df.dropna(subset=['user_input', 'bot_output'], inplace=True)

# Clean text columns
def clean_text(text):
    import re
    if pd.isnull(text):
        return ""
    text = str(text).lower()                           # Convert to lowercase string
    text = re.sub(r'\s+', ' ', text).strip()           # Replace multiple spaces with single space and strip
    text = re.sub(r'[^\w\s.,!?]', '', text)            # Remove characters except word chars, whitespace and punctuations
    return text

# Apply cleaning function to user_input and bot_output columns
df['user_input_clean'] = df['user_input'].apply(clean_text)
df['bot_output_clean'] = df['bot_output'].apply(clean_text)

# Calculate character lengths on cleaned text
df['user_input_length'] = df['user_input_clean'].apply(len)
df['bot_output_length'] = df['bot_output_clean'].apply(len)

# Confirm updated column names
df.columns

# Summary
# - Renamed columns for consistent snake_case formatting
# - Dropped rows missing key text data
# - Defined and applied a regex-based text cleaning function
# - Created new columns for cleaned text versions
# - Calculated and stored character lengths of cleaned text inputs and outputs

# Short Description:
# This block performs text preprocessing by cleaning user and bot text, standardizing column names, removing incomplete rows, and analyzing text length distributions as part of EDA.

# Key Features:
# - Column renaming to snake_case
# - Removal of rows with missing essential text data
# - Regex-based text cleaning preserving basic punctuation
# - Length calculation of cleaned text data for further analysis

# Models/Tools Used:
# - pandas for data manipulation
# - re for regex operations inside the cleaning function

# 📦 Commands / Functions Used:

# pandas
# df.rename(columns=dict, inplace=True)   → Rename DataFrame columns in place
# df.dropna(subset=list)                  → Drop rows where any subset columns have missing values
# df.columns                             → Get list of DataFrame column names
# .apply(function)                      → Apply a function element-wise to a Series

# Python & Regex Utilities
# pd.isnull(value)                     → Check if value is NaN or None
# re.sub(pattern, replacement, text)  → Substitute regex pattern with replacement in text
# str(text).lower()                    → Convert text to lowercase string
# len(text)                           → Get length of text string

# 📏 Step 1: Create new columns to check the length of user and bot responses
df['user_input_length'] = df['user_input'].apply(len)       # Count characters in user query
df['bot_output_length'] = df['bot_output'].apply(len)       # Count characters in bot response

# 📊 Step 2: Summary statistics of the length columns
df[['user_input_length', 'bot_output_length']].describe()   # Basic stats like mean, min, max, std

# 🧮 Step 3: Plotting histogram to visualize character count distribution
plt.figure(figsize=(12, 5))  # Set the figure size for the plot

# 📉 Plot 1: Histogram for user input length
plt.subplot(1, 2, 1)                                       # Left plot
sns.histplot(df['user_input_length'], bins=20, kde=True)   # Histogram with density
plt.title("User Input Length Distribution")                # Title
plt.xlabel("Character Count")                              # X-axis label

# 📈 Plot 2: Histogram for bot response length
plt.subplot(1, 2, 2)                                        # Right plot
sns.histplot(df['bot_output_length'], bins=20, kde=True)    # Histogram with density
plt.title("Bot Output Length Distribution")                 # Title
plt.xlabel("Character Count")                               # X-axis label

# 🎨 Finalize layout and show the combined plots
plt.tight_layout()
plt.show()

# Summary
# - Calculated character lengths of user and bot text inputs
# - Generated descriptive statistics for length columns to understand distribution
# - Visualized length distributions with side-by-side histograms using Seaborn and Matplotlib

# Short Description:
# This block quantifies and visualizes the character length of user queries and bot responses to aid in understanding text size distribution and spotting anomalies.

# Key Features:
# - Length calculation of text fields
# - Summary statistics for length insights
# - Side-by-side histograms with density overlays for visual analysis

# Models/Tools Used:
# - pandas for data manipulation
# - matplotlib.pyplot and seaborn for plotting and visualization

# 📦 Commands / Functions Used:

# pandas
# .apply(len)                          → Applies Python len() to count characters in each text entry
# df.describe()                       → Computes summary statistics for numeric columns

# matplotlib.pyplot
# plt.figure(figsize=(w, h))          → Creates a new figure with specified width and height
# plt.subplot(rows, cols, index)      → Creates subplot in a grid layout
# plt.title(text)                     → Sets the title of the plot
# plt.xlabel(text)                    → Sets the x-axis label
# plt.tight_layout()                  → Adjusts subplot params for a tidy layout
# plt.show()                         → Displays the plot

# seaborn
# sns.histplot(data, bins=n, kde=True) → Plots histogram with kernel density estimate overlay

# Step 4: Check for empty/very short queries or responses (may need handling)
short_inputs = df[df['user_input_length'] < 5]     # User queries < 5 characters
short_outputs = df[df['bot_output_length'] < 5]    # Bot responses < 5 characters

# 📋 Print the results
print("Very short user queries:\n", short_inputs[['user_input']])
print("Very short bot responses:\n", short_outputs[['bot_output']])

# 🧾 Step 5: Optional - Analyze most common words in user inputs
from sklearn.feature_extraction.text import CountVectorizer                        # For word frequency count

# Remove English stopwords and tokenize text
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['user_input'])                                     # Fit to user input column

# 🔢 Sum word frequencies across all rows
word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())  # Convert to dataframe
word_freq_sum = word_freq.sum().sort_values(ascending=False)                       # Sort word count descending

# 📢 Display the top 15 most frequent words
print("🧾 Top 15 Common Words in User Queries:")
print(word_freq_sum.head(15))

# Summary
# - Identified and listed user queries and bot responses with fewer than 5 characters
# - Used CountVectorizer to tokenize user queries and remove English stopwords
# - Computed and displayed the 15 most frequent words in user queries for insight into common terms

# Short Description:
# This block detects unusually short user and bot texts that may require special handling and analyzes the most common words in user queries using tokenization and stopword removal.

# Key Features:
# - Filtering short/empty text entries for potential data quality issues
# - Text tokenization and stopword removal via scikit-learn
# - Frequency analysis of user query vocabulary

# Models/Tools Used:
# - scikit-learn CountVectorizer for text tokenization and frequency analysis
# - pandas for data handling

# 📦 Commands / Functions Used:

# pandas
# df[df['column'] < value]            → Filters rows based on condition for column values
# print(...)                         → Outputs results to console

# scikit-learn CountVectorizer
# CountVectorizer(stop_words='english') → Initializes tokenizer removing English stopwords
# vectorizer.fit_transform(text_series) → Fits and transforms text data to a document-term matrix
# vectorizer.get_feature_names_out()    → Returns list of tokenized feature names
# pd.DataFrame(array, columns=cols)      → Creates DataFrame from array and assigns column names
# .sum().sort_values(ascending=False)   → Sums word counts and sorts descending
# .head(n)                              → Returns top n rows of a Series or DataFrame

"""## Tokenizer Setup & Token Counting"""

# Tokenizer Setup & Token Counting

# 📦 Library Installation: Install pandas, tiktoken, and requests
!pip install pandas tiktoken requests --quiet
# These packages are needed to load data, tokenize text, and make HTTP requests.

# pandas  : For loading and handling tabular datasets (like CSV files).
# tiktoken: Official OpenAI tokenizer library — breaks text into tokens.
# requests: For fetching online content like your Google Sheets CSV file.

# 📥 Import required packages
import pandas as pd           # For handling tabular data
import tiktoken               # For tokenizing text (same as GPT uses)
import requests               # To fetch the dataset from a public URL
from io import StringIO       # To read CSV content from the web as a file

#  STEP 1 — Test Tokenizer on Sample Input (Tamil + English)
print("\n🔍 Step 1: Test Tokenizer on Sample Multilingual Input")

# 📝 Example mixed-language user input (Tamil + English)
sample_input = "நான் one course தேடுகிறேன்"

# 🔠 Load the tokenizer (GPT-compatible)
tokenizer = tiktoken.get_encoding("cl100k_base")

# Confirm tokenizer is loaded successfully
print("Tokenizer loaded successfully:", "cl100k_base")

# 🔡 Tokenize the input text
tokens = tokenizer.encode(sample_input)
print("🧩 Tokens:", tokens)

# 🧮 Token count
print("🔢 Token count:", len(tokens))

# 🪄 Decode each token (optional but useful for analysis)
decoded_tokens = [tokenizer.decode([t]) for t in tokens]
print("🧷 Decoded Tokens:", decoded_tokens)

# 🗒️ Note: Tokenizer splits by subwords, even within Tamil or English words

# Tokenization: Load Dataset and Count Tokens

# 🌐 Step 2: Load the CSV from Google Sheets
# This URL points to your Google Sheet published as a CSV

try:
    csv_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vSMF2sHzDY3eelpyYMwUIFKdfiGijHEADmRGzOsZP6Q_2htfhZ7KKh666LcYvUkiQ/pub?output=csv"  # URL of public Google Sheets CSV
    response = requests.get(csv_url)                     # Send HTTP GET request to fetch CSV data
    response.raise_for_status()                          # Raise HTTPError if the request was unsuccessful
    df = pd.read_csv(StringIO(response.text))            # Read CSV data from the response content into a DataFrame
    print("✅ Dataset loaded successfully.")             # Print success message if loading worked
except Exception as e:
    print("❌ Failed to load dataset:", str(e))          # Print error message if loading failed
    df = pd.DataFrame()                                   # Create empty DataFrame as fallback in case of failure

# Optional: Preview dataset column names
if not df.empty:                                       # Check if DataFrame is not empty
    print("Columns in dataset:", df.columns.tolist())  # Print list of column names
else:                                                  # If DataFrame is empty
    print("Dataset is empty. Skipping further steps.") # Notify that dataset is empty and halt further processing

# Step 3: Set the column that contains the text you want to tokenize
# You can change this to 'User Question' or 'BOT Response' as needed
text_column = 'BOT Response'

if not df.empty and text_column in df.columns:
    # 🔢 Step 4: Tokenize each row and count how many tokens it contains
    # This adds a new column 'token_count' with number of tokens per response
    df['token_count'] = df[text_column].astype(str).apply(lambda x: len(tokenizer.encode(x)))

    # 📊 Step 5: Print dataset token statistics
    # Useful for evaluating if your dataset is too short/long for training or inference
    print(f"\nTotal tokens in '{text_column}': {df['token_count'].sum()}")
    print(f"Average tokens per row: {df['token_count'].mean():.2f}")
    print(f"Max tokens in a row: {df['token_count'].max()}")
    print(f"Min tokens in a row: {df['token_count'].min()}")

    # 🧾 Step 6: Optional – Save the updated dataset with token counts
    # Helpful if you want to review or upload the data later with token info
    df.to_csv("dataset_with_token_counts.csv", index=False)
else:
    print(f"Column '{text_column}' not found or dataset empty, skipping token counting.")

# Summary
# - Installed and imported required packages for data loading and tokenization
# - Tested tokenizer with sample Tamil+English input
# - Loaded dataset from Google Sheets CSV using requests
# - Tokenized specified text column ('BOT Response') and counted tokens per entry
# - Displayed token count statistics to understand dataset token size distribution
# - Saved updated dataset with token counts for future use

# Short Description:
# This block sets up OpenAI's GPT-compatible tokenizer using tiktoken, tests it on multilingual text, loads dataset from a CSV URL, tokenizes text entries, counts tokens, and summarizes token distribution.

# Key Features:
# - Tokenization of mixed-language text inputs
# - Token count calculation per text entry
# - Dataset loading from online CSV source
# - Summary statistics on token counts for analysis
# - Optionally saves dataset with token counts appended

# Models/Tools Used:
# - tiktoken (OpenAI official tokenizer library)
# - pandas (data handling)
# - requests (HTTP requests for data fetching)

# 📦 Commands / Functions Used:

# Python Package Installation
# !pip install package_name --quiet  → Quietly installs required Python packages

# pandas
# pd.read_csv()                      → Reads CSV data into DataFrame
# pd.DataFrame()                    → Creates empty DataFrame (fallback)
# df.to_csv(filename, index=False) → Saves DataFrame to CSV file without index column

# requests
# requests.get(url)                 → Sends HTTP GET request to fetch data
# response.raise_for_status()      → Raises error if HTTP request failed

# tiktoken
# tiktoken.get_encoding("cl100k_base") → Loads GPT tokenizer encoding
# tokenizer.encode(text)            → Converts text string into token IDs list
# tokenizer.decode([token_id])      → Converts token ID back to string

# Python Core
# print()                          → Outputs information to console
# lambda x: len(tokenizer.encode(x)) → Anonymous function to count tokens per text string

# Note on Tokens, Tokenizer, and Tiktoken:
# - Token: A token is a small piece of text (like a word or subword) that NLP models process.
#   For example, "Hello!" might be split into tokens: ["Hello", "!"].
# - Tokenizer: A tool that breaks text into tokens so the model can understand and process it.
# - Tiktoken: OpenAI's official tokenizer library used to tokenize text exactly as GPT models do.

# Filter long Responses:

# 🧪 Step 1: Filter Long Responses (Token Count > 50)
long_responses = df[df['token_count'] > 50]                   # 📍 Filter rows where token count exceeds 50
print("Rows with more than 50 tokens:\n", long_responses)     # 🖨️ Display the filtered rows

# 📊 Step 2: Histogram of Token Counts
import matplotlib.pyplot as plt                               # 📦 Import matplotlib for plotting

df['token_count'].hist(bins=30)                               # 📉 Create histogram with 30 bins using 'token_count'
plt.title("Token Count Distribution")                         # 🏷️ Add plot title
plt.xlabel("Tokens")                                          # ➕ Label x-axis as "Tokens"
plt.ylabel("Number of Responses")                             # ➕ Label y-axis as "Number of Responses"
plt.show()                                                    # 🖼️ Display the histogram

# 💾 Step 3: Save the Final Tokenized Dataset
df.to_csv("tokenized_dataset.csv", index=False)               # 💽 Export DataFrame to CSV without the index column

# Summary
# - Filtered responses where token count is greater than 50 for focused analysis
# - Visualized token count distribution using a histogram plot
# - Saved the processed DataFrame with token counts to CSV for future use

# Short Description:
# This block filters long text responses based on their token counts, visualizes token distribution, and saves the dataset with token information for downstream tasks.

# Key Features:
# - Token count-based filtering for long text analysis
# - Histogram visualization of token count spread
# - Exporting cleaned and token-annotated dataset

# Models/Tools Used:
# - pandas for data filtering and saving
# - matplotlib for data visualization

# 📦 Commands / Functions Used:

# pandas
# df[df['token_count'] > 50]          → Filter rows where token_count exceeds 50
# print(...)                          → Display filtered rows

# matplotlib.pyplot
# df['token_count'].hist(bins=30)         → Plot histogram with 30 bins
# plt.title(), plt.xlabel(), plt.ylabel() → Add plot title and axis labels
# plt.show()                              → Render the plot

# pandas DataFrame
# df.to_csv("filename.csv", index=False) → Save DataFrame to CSV without index column

# Filter and Analyze Long Responses Based on Token Count

# 🧪 Step 1: Check if 'token_count' column exists
if 'token_count' in df.columns:
    # Filter rows where token count exceeds 50
    long_responses = df[df['token_count'] > 50]                   # 📍 Filter long responses
    print(f"Total rows with more than 50 tokens: {len(long_responses)}")  # Summary count
    print("Rows with more than 50 tokens:\n", long_responses)     # 🖨️ Display filtered rows

    # 📊 Step 2: Histogram of Token Counts
    import matplotlib.pyplot as plt                               # 📦 Import matplotlib for plotting

    plt.figure(figsize=(10, 6))                                   # Set figure size for better clarity
    df['token_count'].hist(bins=30)                               # 📉 Create histogram with 30 bins using 'token_count'
    plt.title("Token Count Distribution")                         # 🏷️ Add plot title
    plt.xlabel("Tokens")                                          # ➕ Label x-axis as "Tokens"
    plt.ylabel("Number of Responses")                             # ➕ Label y-axis as "Number of Responses"
    plt.tight_layout()                                            # Adjust layout to prevent overlap
    plt.show()                                                    # 🖼️ Display the histogram

    # 💾 Step 3: Save the Final Tokenized Dataset
    df.to_csv("tokenized_dataset.csv", index=False)               # 💽 Export DataFrame to CSV without the index column
else:
    print("Column 'token_count' not found in DataFrame. Skipping filtering and plotting.")

# Short Description:
# This block filters and analyzes long text responses based on token count and saves the processed dataset.
# It includes the following:
# - Filters rows where the token count exceeds 50 and displays them for review
# - Plots a histogram showing the distribution of token counts across responses
# - Saves the final tokenized dataset to a CSV file named 'tokenized_dataset.csv'

# These steps help in identifying overly long responses, visualizing token usage, and exporting clean data for further use.

"""## Approaches"""

# Installing Required Packages

# 🌐 Install Gradio for building the chatbot UI
!pip install gradio --quiet        # Frontend web app to create chatbot interface quickly

# 🌍 Install langdetect for automatic language detection
!pip install langdetect --quiet    # Detects input language automatically (supports many languages)

# 🤖 Install Hugging Face Transformers for translation models (M2M100)
!pip install transformers --quiet   # Provides pre-trained models for translation and NLP tasks

# ⚙️ Install PyTorch (required by transformers)
# 💡 This installs CPU version. For GPU support, visit https://pytorch.org/get-started/locally/
!pip install torch --quiet          # Backend tensor computation library required for transformers

# | Package        | Purpose                                      |
# | -------------- | -------------------------------------------- |
# | gradio         | Frontend web app for chatbot                 |
# | langdetect     | Auto language detection (e.g., Hindi, Tamil)|
# | transformers   | Load translation models from Hugging Face    |
# | torch          | Tensor computation engine used by models     |

"""### Model Selection & SetUp + Translation Logic"""

# Model Selection & SetUp + Translation Logic

# 📦 Step 1: Import Required Libraries

# 🛠️ OS & Regex utilities
import os                             # OS-level operations
import re                             # Regular expressions for text cleaning

# 🌍 Language Detection
from langdetect import detect         # Auto-detects input language

# 🤖 Hugging Face Transformers
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Translation model and tokenizer

# ⚙️ PyTorch for model inference
import torch                          # Tensor operations and GPU/CPU support

# 🔄 Step 2: Load Multilingual Translation Model (M2M100)

# 🔗 Define model name from Hugging Face
translation_model_name = "facebook/m2m100_418M"                        # Supports 100+ languages

# 🧰 Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(translation_model_name)      # Load tokenizer for M2M100

# 🧠 Load translation model
model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)  # Load M2M100 model

# 🚀 Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Auto-detect GPU/CPU
model.to(device)                                                       # Move model to device


# 🌐 Step 3: Supported Language Codes Dictionary (ISO 639-1)
# 🏷️ Define supported language codes for your application
language_code_map = {
    "English": "en",
    "Tamil": "ta",
    "Hindi": "hi",
    "Telugu": "te",
    "Malayalam": "ml",
    "Kannada": "kn",
    "Marathi": "mr",
    "Gujarati": "gu",
    "Bengali": "bn",
    "Urdu": "ur",
    "French": "fr",
    "German": "de"
}


# 🔁 Step 4: Preprocessing, Detection, Translation & Utilities

# 🧹 Text Preprocessing Function
def clean_text(text):
    """
    Removes extra whitespace and optional symbols/emojis to improve detection and translation.
    """
    # 🔍 Remove extra spaces
    text = re.sub(r'\s+', ' ', text.strip())               # Clean whitespace
    # 🔍 Optionally remove emojis/special characters
    text = re.sub(r'[^\w\s.,!?]', '', text)                # Remove unwanted symbols
    return text                                            # Return cleaned text


# 🔍 Language Detection Function
def detect_language(text):
    """
    Detects the language of the given input text using langdetect.
    Returns ISO 639-1 language code or defaults to 'en' on failure.
    """
    try:                                                   # 🛡️ Start error handling block
        return detect(text)                                # 🔎 Try detecting language
    except:                                                # 🛑 On failure
        return "en"                                        # Fallback to English


# 🌐 Translate to English
def translate_to_english(text, source_lang_code):
    """
    Translates input text from the detected source language to English using M2M100.
    Includes input cleanup, error handling, and optional debug print.
    """
    try:                                                           # 🛡️ Start error handling block
        # 🧹 Clean the text
        text = clean_text(text)                                    # Clean input text
        # 🌍 Set source language
        tokenizer.src_lang = source_lang_code                      # Set tokenizer source language
        # 🧱 Tokenize & encode text
        encoded = tokenizer(text, return_tensors="pt").to(device)  # Encode input on device
        # 🚀 Generate translation output (target: English)
        generated_tokens = model.generate(
            **encoded,
            forced_bos_token_id=tokenizer.lang_code_to_id["en"]    # Force output lang as English
        )
        # 🔄 Decode translated output
        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]  # Decode tokens
        # 🪵 Debug log (optional)
        print(f"[{source_lang_code}] {text} → [en] {translated_text}")  # Log translation
        return translated_text                                          # Return translated text
    except Exception as e:                                              # 🛑 Catch exceptions
        return f"[Translation Error to English] {str(e)}"               # Return error message


# 🌐 Translate from English back to original language
def translate_from_english(text, target_lang_code):
    """
    Translates GPT's English response back into the user's original language using M2M100.
    Includes input cleanup, error handling, and optional debug print.
    """
    try:                                                           # 🛡️ Start error handling block
        # 🧹 Clean the text
        text = clean_text(text)                                    # Clean English response text
        # 🌍 Set source language to English
        tokenizer.src_lang = "en"                                  # Set source lang to English
        # 🧱 Tokenize & encode English response
        encoded = tokenizer(text, return_tensors="pt").to(device)  # Encode input on device
        # 🚀 Generate translation to user's language
        generated_tokens = model.generate(
            **encoded,
            forced_bos_token_id=tokenizer.lang_code_to_id[target_lang_code]  # Force output lang
        )
        # 🔄 Decode translated output
        translated_response = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]  # Decode tokens
        # 🪵 Debug log (optional)
        print(f"[en] {text} → [{target_lang_code}] {translated_response}")  # Log translation
        return translated_response                                          # Return translated text
    except Exception as e:                                                  # 🛑 Catch exceptions
        return f"[Translation Error from English] {str(e)}"                 # Return error message


# 🔢 Utility: Count tokens in input (for GPT prompt length check)
def count_tokens(text):
    """
    Returns the number of tokens in a given text using the tokenizer.
    Useful to monitor token limits before sending to GPT.
    """
    tokens = tokenizer.encode(text, return_tensors="pt")    # 🧾 Tokenize input text
    return len(tokens[0])                                   # 📏 Return token count

# Short Description:
# This script sets up a multilingual chatbot translation pipeline using the Facebook M2M100 model.
# It includes:
# - Automatic language detection of user input text with fallback to English
# - Text preprocessing to clean and normalize input for better translation accuracy
# - Translation from detected language to English (for GPT processing)
# - Translation from English back to the user's original language (for response)
# - Error handling in translation functions to ensure robustness
# - A utility function to count tokens in input text to manage GPT input limits
# - Debug print statements (optional) to trace translation flow and outputs
#
# This enables building a multilingual chatbot that can understand queries in multiple languages,
# translate them to English for processing, and respond back in the user's language seamlessly.


# Commands / Functions Used:

# Import Libraries
# import os, re                      → For OS operations and regex text cleaning
# from langdetect import detect      → To detect input text language automatically
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
#                                 → Load tokenizer and seq2seq translation model
# import torch                      → For tensor operations and GPU acceleration

# Model & Tokenizer Loading
# AutoTokenizer.from_pretrained(model_name)
# AutoModelForSeq2SeqLM.from_pretrained(model_name)
# torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Language Detection
# detect(text)                     → Returns ISO 639-1 language code for input text

# Text Cleaning
# re.sub()                        → Remove extra whitespace and unwanted symbols

# Translation
# tokenizer(text, return_tensors="pt").to(device)
# model.generate(...)
# tokenizer.batch_decode(..., skip_special_tokens=True)

# Token Counting
# tokenizer.encode(text, return_tensors="pt")
# len(tokens[0])

# Print Statements
# print()                        → For debug logs to monitor translations and tokens

# Notes:
# - 'forced_bos_token_id' parameter in model.generate() ensures correct target language output.
# - Cleaning text before detection and translation improves accuracy.
# - Error handling prevents failures during translation in live apps.
# - Token counting helps monitor GPT prompt sizes to avoid exceeding limits.

"""### Fine-Tuning"""

# 📦 Install Required Python Libraries for Model Fine-Tuning

# Install required Python libraries quietly (minimal log output)
!pip install sentence-transformers pandas --quiet

# 📦 Libraries Installed and Their Purpose
# 🧠 sentence-transformers → For loading, training, and fine-tuning transformer-based sentence embedding models
# 🐼 pandas                → For reading and processing tabular datasets (Google Sheets CSV in our case)

# Summary
# - Installed essential libraries for semantic search model fine-tuning
# - Used the --quiet flag to keep installation logs minimal in Colab/Notebook
# - sentence-transformers provides tools to train models for text similarity and semantic search
# - pandas is used for loading and preprocessing the GUVI FAQ dataset

# Key Features:
# - Load pre-trained sentence embedding models from Hugging Face
# - Fine-tune models with domain-specific question-answer pairs
# - Process and clean CSV/Excel datasets easily

# Commands / Functions Used:
# !pip install package_name --quiet  → Installs Python packages silently
# pandas.read_csv(url)               → Loads CSV data from a given URL
# SentenceTransformer()               → Loads pre-trained or fine-tuned sentence embedding models

# 🎯 Step: Fine-Tune SentenceTransformer Model with GUVI Dataset

# 📦 1. Import Required Libraries
import os
os.environ["WANDB_DISABLED"] = "true"  # 🚫 Disable Weights & Biases logging to avoid login prompt

from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import pandas as pd

# 📄 Short Description:
# - Loads a pre-trained sentence embedding model
# - Reads GUVI FAQ dataset from Google Sheets (CSV format)
# - Prepares question-answer pairs for training
# - Fine-tunes the model for better semantic similarity on GUVI-specific queries
# - Saves the custom fine-tuned model locally for later use


# 🧠 2. Load Base Pre-trained Model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
# Purpose:
# - Provides a starting point for training
# - 'all-MiniLM-L6-v2' is lightweight and optimized for semantic similarity tasks


# 📥 3. Load GUVI FAQ Dataset
df = pd.read_csv(
    "https://docs.google.com/spreadsheets/d/e/2PACX-1vSMF2sHzDY3eelpyYMwUIFKdfiGijHEADmRGzOsZP6Q_2htfhZ7KKh666LcYvUkiQ/pub?output=csv"
)
df = df.dropna(subset=["User Question", "BOT Response"])  # 🧹 Remove rows with missing Q/A


# 🏗️ 4. Prepare Training Examples
train_examples = [
    InputExample(texts=[row['User Question'], row['BOT Response']])
    for _, row in df.iterrows()
]
# Each InputExample → (sentence1, sentence2) for similarity learning


# 📦 5. Create DataLoader for Batching
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
# Purpose:
# - Loads data in batches for efficient training
# - Shuffles to prevent model from memorizing sequence

# 🎯 6. Define Loss Function
train_loss = losses.MultipleNegativesRankingLoss(model)
# Purpose:
# - Maximizes similarity between correct Q/A pairs
# - Minimizes similarity with incorrect ones in the batch


# 🚀 7. Fine-tune the Model
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3,
    warmup_steps=100
)
# Purpose:
# - Trains the model on GUVI-specific Q/A patterns
# - Warmup helps model adapt gradually

# 💾 8. Save the Fine-Tuned Model
model.save("guvi_semantic_model")
print("✅ Fine-tuning complete. Model saved in 'guvi_semantic_model'")


# 📦 Commands / Functions Used:
# os.environ["WANDB_DISABLED"] = "true" → Disables Weights & Biases tracking
# pandas.read_csv(url)                  → Loads dataset from Google Sheets CSV
# SentenceTransformer(model_name)       → Loads pre-trained embedding model
# InputExample(texts=[q, a])             → Creates training pair for semantic learning
# DataLoader(dataset, batch_size, ...)  → Efficient batch data loading
# losses.MultipleNegativesRankingLoss() → Optimizes semantic similarity training
# model.fit(...)                         → Runs the fine-tuning process
# model.save(path)                       → Saves trained model locally

# 📌 Use Cases:
# - Improves chatbot’s ability to match GUVI-related questions & answers
# - Handles paraphrased queries better (e.g., "Zen class details" vs "What is GUVI Zen class?")
# - Can be integrated into semantic search, RAG, or FAQ systems

"""## Gradio Interface (Hugging Face)"""

# 📦 Install Required Libraries for the GUVI Multilingual FAQ Chatbot
# These commands install essential packages for NLP, translation, and fuzzy matching.

# Install PyTorch silently (for model inference and tensor operations)
!pip install torch --quiet        # PyTorch: core deep learning framework

# Install Deep Translator library (for language translation)
!pip install deep-translator      # Deep Translator: multilingual translation API wrapper

# Install fuzzywuzzy with speedup (for fast fuzzy string matching)
!pip install fuzzywuzzy[speedup]  # FuzzyWuzzy: string similarity & matching

# Install faiss-cpu for efficient similarity search and clustering (optional but recommended for scaling)
!pip install faiss-cpu             # FAISS: efficient similarity search library


# 📦 Libraries Installed and Their Purpose
# 🐍 torch               → PyTorch deep learning framework for model training and inference
# 🌐 deep-translator     → API wrapper for multilingual text translation (supports many languages)
# 🔍 fuzzywuzzy[speedup] → Fast string matching and similarity scoring for fuzzy text matching
# ⚡ faiss-cpu           → Efficient similarity search and clustering for large datasets

# 🧪 Test: Language Detection & Translation Pipeline

# 🧼 Utility Function to Clean Input
def clean_text(text):
    """
    Removes extra whitespaces, invisible Unicode characters, and normalizes the input.
    """
    text = re.sub(r'\s+', ' ', text)        # 🔍 Replace multiple spaces/newlines with a single space for normalization
    text = text.strip()                     # ✂️ Remove leading and trailing whitespace
    return text

# Step-by-Step Test Again with Cleaned Input
sample = "வணக்கம் நண்பா, எப்படி இருக்கீங்க?"            # 📝 Tamil input example
sample_cleaned = clean_text(sample)                   # 🧹 Clean the input text

lang_code = detect_language(sample_cleaned)           # 🌍 Detect language code (ISO 639-1)
translated = translate_to_english(sample_cleaned, lang_code)  # 🤖 Translate cleaned text to English using M2M100 model

# 🖨️ Final Output
print("Detected Language:", lang_code)                 # 📣 Expect 'ta' for Tamil
print("Translated to English:", translated)            # 📣 Expect English translation like "Hello friend, how are you?"

# 📄 Short Description:
# - Cleans user input to remove noise and normalize text
# - Detects input language using langdetect
# - Translates detected language text into English with M2M100 model
# - Demonstrates end-to-end multilingual pipeline for chatbot input handling

# 📦 Commands / Functions Used:

# Python & Regex Utilities
# re.sub(r'\s+', ' ', text) → Normalize multiple whitespace characters into a single space
# text.strip()              → Strip leading and trailing whitespace
# print(...)                → Display output (detected language and translation)

# 🌍 Language Detection – langdetect
# detect_language(text)     → Detects language code (ISO 639-1), e.g., 'ta', 'en'

# 🤖 Hugging Face Transformers – transformers
# tokenizer.src_lang = source_lang_code                 → Set source language before translation
# tokenizer(text, return_tensors="pt")                  → Tokenize and convert to PyTorch tensor
# model.generate(...)                                   → Generate translated token IDs
# tokenizer.batch_decode(..., skip_special_tokens=True) → Decode tokens to readable text

# ⚙️ PyTorch – torch
# .to(device)               → Move tensor to GPU/CPU depending on availability

# Use Cases:
# - Validate translation accuracy
# - Debug language detection and translation steps
# - Integrate multilingual input support into chatbots or NLP applications

faq_data = [
    {"question": "Premium Pass?", "answer": "The Premium Pass offers access to a variety of courses and exclusive resources on GUVI."},
]

user_question = "Premium Pass?"
response = chatbot_response(user_question, faq_data)
print(response)  # Output: Premium Pass offers access to ...

# 📄 GUVI Multilingual FAQ Chatbot

# 📦 1. Import Required Libraries
from langdetect import detect                                # 🔍 Language detection for multilingual input
from deep_translator import GoogleTranslator                 # 🌐 Translation between languages
from sentence_transformers import SentenceTransformer, util  # 🧠 Semantic similarity & encoding
import torch                                                 # ⚙️ PyTorch for tensor operations and device setup
import pandas as pd                                          # 🐼 For loading and cleaning FAQ dataset
import gradio as gr                                          # 🖥️ Web-based chatbot interface
import re                                                    # 🔎 Regex utilities for cleaning and splitting input
from transformers import pipeline                            # 🤖 Transformers pipeline for QA and GPT fallback
from fuzzywuzzy import fuzz, process                         # 🔡 Fuzzy matching utilities for text similarity
import logging                                               # 📁 For logging chatbot interactions

# ⚙️ 2. Device Setup (CPU/GPU)
device = 'cuda' if torch.cuda.is_available() else 'cpu'      # 🔄 Check if GPU is available; fallback to CPU
print(f"Device set to use: {device}")                        # 🖨️ Display device info

# 🗂️ 3. Define GUVI Keywords and Synonyms List
# guvi_keywords ->📝 List of keywords, synonyms, and typos related to GUVI

# 🗂️ Define GUVI Keywords + Synonyms + Short Forms

# Core brand/platform & variations
guvi_brand = [
    "guvi", "guvvi", "govy", "govvi", "guvee", "guvy", "gubi", "guv",
    "zen class", "zenclass", "zen clss", "zenclss", "zen",
    "iitm", "iit madras", "iit-madras", "iit madrs", "iitmadras",
    "nasscom","premium pass"
    "smriti mandhana"
]

# Courses and skills keywords
guvi_courses = [
    "full stack", "fullstack", "full stak", "fstack", "fsd", "fulstck",
    "data science", "datascience", "data scince", "ds", "dta sc", "data sci",
    "machine learning", "machinelearning", "machin learn", "ml", "mach learn",
    "ai", "artificial intelligence", "artificialintelligence", "artfcl intlgence",
    "devops", "cloud computing", "blockchain", "cybersecurity"
]

# Events & activities
guvi_events = [
    "skillathon", "skilathon", "skill athon", "hackathon", "hakathon"
]

# Tools, features & certificates
guvi_tools = [
    "guvi ide", "guviide", "ide", "guvi certificate", "guvi cert",
    "certificate", "certificates", "cert", "cerficate",
    "placement", "placements", "job guarantee", "mentorship", "career guidance",
    "job assistance", "emi", "installments"
]

# Common queries & variations
guvi_queries = [
    "python course", "python courses", "py course", "java course", "java courses",
    "course fee", "course fees", "course cost", "fees", "pricing", "cost", "pric", "feer",
    "login", "sign up", "signup", "sign-in", "signin", "register", "registration",
    "guvi login", "login guvi", "guvi log in"
]

# Combine all keywords into one master list if needed
guvi_keywords = guvi_brand + guvi_courses + guvi_events + guvi_tools + guvi_queries


# 📥 4. Load FAQ Dataset and Clean
faq_df = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vSMF2sHzDY3eelpyYMwUIFKdfiGijHEADmRGzOsZP6Q_2htfhZ7KKh666LcYvUkiQ/pub?output=csv")  # 📂 Load CSV data
faq_df.dropna(inplace=True)                                                     # ❌ Remove rows with missing data
faq_df['User Question'] = faq_df['User Question'].str.strip().str.lower()       # 🔄 Normalize question text
faq_df['BOT Response'] = faq_df['BOT Response'].str.strip().str.lower()         # 🔄 Normalize response text
faq_df.drop_duplicates(subset=['User Question', 'BOT Response'], inplace=True)  # 🧹 Remove duplicates

faq_questions = faq_df['User Question'].tolist()                                # 📝 Extract questions as list
faq_answers = faq_df['BOT Response'].tolist()                                   # 📝 Extract answers as list

# 💡 5. Load Semantic Search Model and Generate Embeddings
try:
    model = SentenceTransformer('guvi_semantic_model', device=device)
except Exception as e:
    print(f"❌ Error loading semantic search model: {e}")                       # 🛑 Error handling for model load
    model = None

if model:
    faq_embeddings = model.encode(faq_questions, convert_to_tensor=True)         # 🔢 Compute embeddings for FAQ
else:
    faq_embeddings = None

# 🧭 6. Named Entity Mapping for Local Languages
entity_map = {                                               # 🗺️ Map local language terms to English named entities
    "குவி": "GUVI",
    "ஐஐடி மெட்ராஸ்": "IIT-Madras",
    "गुवी": "GUVI",
    "गुवि": "GUVI",
    "গুভি": "GUVI",
    "ఇదు": "GUVI",
    "ಗುವಿ": "GUVI",
    "ഐടി മദ്രാസ്": "IIT-Madras",
    "आयआयटी मद्रास": "IIT-Madras",
    "iitm": "IIT-Madras",
    "iit madras": "IIT-Madras",
    "guvi zen class": "GUVI Zen Class",
    "गेवि़": "GUVI",
}

# 🔧 7. Preserve Named Entities Before Translation
def preserve_named_entities(text, lang):
    if lang in ['ta', 'hi', 'bn']:                          # 🔎 Check if language is Tamil, Hindi, or Bengali
        for native, eng in entity_map.items():              # 🔄 Replace native terms with English equivalents
            text = text.replace(native, eng)
    return text

# 8. Input Validation Function
def is_valid_input(text):
    if not text or len(text.strip()) < 3:                   # ❌ Reject empty or too short input
        return False, "❗ Please enter a valid question, not just numbers or symbols."

    if re.fullmatch(r"[0-9\s\W_]+", text):                  # ❌ Reject inputs with only numbers/symbols
        return False, "❗ Please enter a valid question, not just numbers or symbols."

    if len(text.strip()) <= 5:                              # ❌ Reject too short meaningful input
        return False, "Sorry, I couldn't understand your question. Please rephrase or contact 📧 support@guvi.in for help."

    if re.fullmatch(r"[a-zA-Z]+", text.strip()) and len(text.strip()) < 10:  # ❌ Reject short alphabetic-only strings
        return False, "Sorry, I couldn't understand your question. Please rephrase or contact 📧 support@guvi.in for help."

    return True, ""

def find_best_faq_answer(user_question, faq_data):
    user_question_lower = user_question.strip().lower()
    for entry in faq_data:
        if entry['question'].strip().lower() == user_question_lower:
            return entry['answer']
    return None

def chatbot_response(user_input, faq_data):
    # Step 1: Validate input
    is_valid, validation_msg = is_valid_input(user_input)
    if not is_valid:
        return validation_msg  # Return fallback message immediately if invalid input

    # Step 2: Search for best matching FAQ answer
    answer = find_best_faq_answer(user_input, faq_data)  # You write this function to search your FAQ

    # Step 3: Return answer if found, else fallback message
    if answer:
        return answer
    else:
        return "Sorry, I can only answer questions related to GUVI. Please contact 📧 support@guvi.in for other queries."

# GUVI relevance check function
def is_guvi_related(question):
    question_lower = question.lower()                                   # 🔍 Convert to lowercase for comparison
    return any(keyword in question_lower for keyword in guvi_keywords)  # ✅ Return True if any keyword found

# 🔍 9. Semantic Search Function with Threshold + Suggestion
def find_best_answer(question_en, threshold=0.55):
    try:
        question_embedding = model.encode(question_en, convert_to_tensor=True)  # 🔢 Encode user question
        similarity_scores = util.pytorch_cos_sim(question_embedding, faq_embeddings)[0]  # ⚖️ Compute cosine similarity
        top_score = torch.max(similarity_scores).item()               # 🔝 Highest similarity score
        top_result = torch.argmax(similarity_scores).item()           # 🏆 Index of best matching FAQ

        print(f"🔍 Top Match Score: {top_score:.4f}")                 # 🖨️ Log similarity score
        print(f"📌 Suggested Question: {faq_questions[top_result]}")  # 🖨️ Log suggested FAQ

        if top_score >= threshold:                                    # ✅ If similarity above threshold
            return faq_answers[top_result]                            # 🔄 Return matching answer
        else:                                                         # ⚠️ Below threshold, return suggestion
            suggested_question = faq_questions[top_result]
            suggested_answer = faq_answers[top_result]
            return (f"⚠️ Couldn't find an exact match.\n"
                    f"Did you mean: \"{suggested_question}\"?\n\n"
                    f"💬 Answer: {suggested_answer}")

    except Exception as e:
        print(f"❌ Error in semantic search: {e}")                    # 🛑 Error handling
        return "Sorry, I couldn't understand your question. Please rephrase or contact 📧 support@guvi.in for help."

# 🔁 10. Retrieval Augmented Generation (RAG) Setup (Index & QA Pipeline)
import faiss                                                    # 🔎 Fast similarity search library
import numpy as np                                              # 🔢 Array handling

# Try loading RAG components with error handling
try:
    rag_model = SentenceTransformer('guvi_semantic_model')
    rag_corpus = [                                                             # 📚 Small knowledge base
        "GUVI is an edtech platform partnered with IIT-Madras.",
        "It offers full-stack, data science, AI, DevOps, and more.",
        "GUVI supports Tamil, Hindi, and other Indian languages.",
        "Smriti Mandhana is GUVI’s brand ambassador.",
        "GUVI offers Zen class for hands-on learning and practice."
    ]
    rag_embeddings = rag_model.encode(rag_corpus, convert_to_numpy=True) # 🔢 Compute corpus embeddings
    index = faiss.IndexFlatL2(rag_embeddings.shape[1])                   # ⚙️ Create FAISS L2 index
    index.add(rag_embeddings)                                            # ➕ Add embeddings to index

    qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2", device=0 if device == 'cuda' else -1)  # 🤖 Load QA model

except Exception as e:
    print(f"❌ Error loading RAG or QA pipeline: {e}")                   # 🛑 Handle load errors
    rag_model, index, qa_pipeline = None, None, None

# RAG fallback function
def rag_fallback(query_en):
    if not rag_model or not index or not qa_pipeline:                    # ❌ Check availability
        return "⚠️ RAG fallback system is not available right now."
    try:
        query_vector = rag_model.encode(query_en, convert_to_numpy=True).reshape(1, -1)  # 🔢 Encode query
        D, I = index.search(query_vector, k=2)                                        # 🔍 Search top 2 similar contexts
        context = " ".join([rag_corpus[i] for i in I[0]])                             # 📚 Combine contexts
        result = qa_pipeline(question=query_en, context=context)                      # 🤖 Get answer from QA model
        if result['score'] > 0.3 and result['answer'].strip() != "":                  # ✅ Check confidence and non-empty answer
            return result['answer'].strip()                                           # 🔄 Return answer
        else:
            return "Sorry, no relevant answer found."
    except Exception as e:
        print("❌ RAG Error:", str(e))                                                # 🛑 Handle errors
        return "Sorry, I couldn't find a relevant answer."

# 🤖 11. GPT-Style Text Generation Fallback with FLAN-T5
gpt_generator = pipeline("text2text-generation", model="google/flan-t5-base", device=0 if device == 'cuda' else -1)  # 🤖 Load FLAN-T5 text generation model

def gpt_fallback(question):
    try:
        # 🔧 Controlled prompt with GUVI context (omitted here for brevity)
        prompt = f"...User Question: {question}\nGUVI Assistant:"
        result = gpt_generator(prompt, max_length=150, temperature=0.3, top_p=0.9)[0]['generated_text']  # 📝 Generate response
        return result.strip()
    except Exception as e:
        print("❌ GPT Fallback Error:", str(e))                  # 🛑 Error handling for GPT generation
        return "Sorry, I couldn't generate a helpful response."

# ✂️ 12. Split Multiple Questions from Input
def split_questions(user_input):
    parts = re.split(r'\?|&| and |\n', user_input)                # 🔪 Split by delimiters (?, &, and, newline)
    return [part.strip() for part in parts if part.strip()]       # 🔄 Return cleaned list of questions

# 📄 13. File-based Logging Setup
logging.basicConfig(
    filename='chatbot_interactions.log',                       # 📁 Log file path
    filemode='a',                                              # ➕ Append mode
    format='%(asctime)s | %(levelname)s | %(message)s',        # 📝 Log format
    level=logging.INFO                                         # ℹ️ Log level INFO
)

def log_interaction(question, guvi_related, fallback_used):
    status = "GUVI" if guvi_related else "Non-GUVI"             # 🔖 Tag question category
    logging.info(f"Question: {question} | Category: {status} | Fallback: {fallback_used}")  # 📥 Log details

# 🧠 14. Main Multilingual FAQ Chatbot Logic
def multilingual_qa_bot(user_input):
    print("Raw User Input:", user_input)                        # 🖨️ Log raw input

    try:
        overall_lang = detect(user_input)                      # 🌍 Detect input language
        if overall_lang not in ['ta', 'hi', 'bn', 'en']:       # ❌ Limit supported languages
            overall_lang = 'en'
    except:
        overall_lang = "en"                                     # 🛑 Default to English on detection fail

    user_input_fixed = preserve_named_entities(user_input, overall_lang)  # 🔄 Preserve named entities
    question_list = split_questions(user_input_fixed)           # ✂️ Split multiple questions

    answers = []
    for q in question_list:
        is_valid, validation_msg = is_valid_input(q)           # ✅ Validate input question
        if not is_valid:
            answers.append(f"👉 {validation_msg}")             # 🛑 Append validation failure message
            log_interaction(q, guvi_related=False, fallback_used="invalid_input")  # 📥 Log interaction
            continue

        try:
            q_lang = detect(q)                                  # 🌍 Detect language per question
            if q_lang not in ['ta', 'hi', 'bn', 'en']:
                if re.search(r"[a-zA-Z]", q):
                    q_lang = 'en'
                else:
                    q_lang = overall_lang
        except:
            q_lang = overall_lang

        q_fixed = preserve_named_entities(q, q_lang)           # 🔄 Preserve named entities again
        q_en = GoogleTranslator(source=q_lang, target="en").translate(q_fixed) if q_lang != "en" else q_fixed  # 🌐 Translate question to English

        guvi_flag = is_guvi_related(q_en)                      # 🔎 Check if question is GUVI-related
        if not guvi_flag:
            answers.append("👉 Sorry, I can only answer questions related to GUVI. Please contact 📧 support@guvi.in for other queries.")
            log_interaction(q, guvi_related=False, fallback_used="non_guvi_query")
            continue

        en_answer = find_best_answer(q_en)                      # 🔍 Semantic FAQ search
        fallback_used = "semantic_search"

        if en_answer.lower().startswith("⚠️ couldn't find an exact match") or "couldn't understand" in en_answer.lower():
            en_answer = rag_fallback(q_en)                      # 🔁 RAG fallback
            fallback_used = "RAG_fallback"
            if not en_answer or "no relevant answer" in en_answer.lower():
                en_answer = gpt_fallback(q_en)                  # 🤖 GPT fallback
                fallback_used = "GPT_fallback"

        try:
            final_answer = GoogleTranslator(source='en', target=q_lang).translate(en_answer) if q_lang != "en" else en_answer  # 🌐 Translate back to user language
        except:
            final_answer = en_answer                            # 🛑 On failure, use English

        answers.append(f"👉 {final_answer}")                    # 💬 Append final answer

        log_interaction(q, guvi_related=True, fallback_used=fallback_used)  # 📥 Log successful interaction

    return "\n\n".join(answers)                                 # 🔄 Return all answers combined

# 🌐 15. Gradio Interface Setup for Web Chatbot
demo = gr.Interface(
    fn=multilingual_qa_bot,                                     # 🧠 Function to call on input
    inputs=gr.Textbox(lines=3, placeholder="Ask multiple questions about GUVI (English, Tamil, etc)..."),  # ✍️ Input box
    outputs="text",                                             # 🖥️ Output display as text
    title="GUVI Multilingual FAQ Chatbot 🤖",                   # 🏷️ UI title
    description="Ask multiple questions in English, Tamil, Hindi, etc.\nExample:\nWho is the brand ambassador of GUVI? & How long is the GUVI DevOps course?"  # ℹ️ Description
)

# 🚀 16. Launch the Gradio Web App
if __name__ == "__main__":
    demo.launch(share=True, debug=True)                       # 🌍 Launch app with sharing and debugging enabled


# 📄 GUVI Multilingual FAQ Chatbot – Summary & Key Details

# 📝 Short Description:
# This chatbot system supports multilingual user queries related to GUVI using a layered approach:
# 1) Detects user language and translates queries to English.
# 2) Performs semantic similarity search over a curated FAQ dataset
#    to find the best matching question-answer pair using cosine similarity and a defined threshold.
# 3) If FAQ match is weak, uses a Retrieval Augmented Generation (RAG) QA fallback with a small knowledge corpus.
# 4) If still no answer, generates response using GPT-based FLAN-T5 model with strict prompt controls.
# 5) Translates final answers back to user’s language.
# 6) Logs all interactions for monitoring and improvement.
# It supports multiple Indian languages and handles typos, synonyms, and named entities gracefully.

# 🛠️ Key Features Implemented:
# - Multilingual input support with automatic language detection and bi-directional translation
# - Named entity preservation for local language terms before translation
# - Semantic search over FAQ dataset with cosine similarity threshold and fallback suggestion
# - Retrieval Augmented Generation (RAG) QA fallback using FAISS index and RoBERTa QA model
# - GPT fallback with FLAN-T5 text generation, constrained by official GUVI context to prevent hallucination
# - Input validation and question splitting for handling multiple queries at once
# - File-based logging of user questions, categories, and fallback methods for analytics
# - Web-based UI using Gradio for easy interaction and deployment
# - Extensive error handling and graceful fallbacks to maintain user experience

# 🧰 Tools / Libraries Used:
# - langdetect            : Language identification from user input text
# - deep_translator       : GoogleTranslator for language translation
# - sentence_transformers : For embedding FAQ questions and semantic similarity search
# - faiss                 : Efficient similarity search and indexing for RAG fallback
# - transformers          : Hugging Face pipelines for question answering and text generation
# - torch                 : PyTorch backend for model inference and GPU acceleration
# - pandas                : Data manipulation for FAQ loading and cleaning
# - gradio                : Web app frontend for chatbot interface
# - fuzzywuzzy            : Fuzzy matching utilities for text similarity
# - logging               : File logging of chatbot interactions
# - re                    : Regex utilities for input cleaning and question splitting

# 🔧 Important Commands / Functions Used:
# - detect(text)                                   : Language detection
# - GoogleTranslator(source, target).translate()  : Language translation
# - SentenceTransformer(model_name).encode()      : Embed sentences for semantic similarity
# - util.pytorch_cos_sim()                         : Compute cosine similarity between embeddings
# - faiss.IndexFlatL2() and index.search()        : Efficient nearest neighbor search for RAG
# - pipeline("question-answering")                 : RoBERTa-based QA model for RAG fallback
# - pipeline("text2text-generation")               : FLAN-T5 for GPT-style text generation fallback
# - logging.basicConfig() and logging.info()       : Logging configuration and usage
# - re.split()                                     : Split multiple user questions
# - gr.Interface() and demo.launch()               : Gradio UI setup and launch

# ⚙️ Customizations Done:
# - Created extensive GUVI-specific keyword list with synonyms, typos, and short forms for relevance filtering
# - Developed named entity mapping for multiple Indian languages to preserve important brand/course names during translation
# - Integrated multiple fallback layers (semantic FAQ, RAG QA, GPT generation) with priority order and thresholding
# - Implemented input validation to filter out invalid, too short, or irrelevant questions
# - Designed controlled GPT prompts enforcing strict context adherence to avoid hallucination and ensure factuality
# - Added support for splitting multiple questions in one input, processing them individually, and returning combined answers
# - Implemented robust error handling around translation, model loading, and inference to ensure stability
# - Enabled detailed logging of user interactions including fallback methods for future analysis/improvements
# - Built easy-to-use web interface via Gradio with multi-line input and clear instructions

# Summary:
# This project builds a robust, scalable, and user-friendly multilingual chatbot for GUVI-related FAQs using state-of-the-art NLP techniques.
# The layered approach combines semantic search, retrieval QA, and generative fallback to ensure high answer coverage and accuracy.
#
# # Semantic Search Best Match Mechanism:
# - Uses SentenceTransformer embeddings to encode FAQ questions and user queries.
# - Computes cosine similarity between user query embedding and FAQ embeddings.
# - Finds the best matching FAQ question based on highest similarity score.
# - Uses a configurable similarity threshold to decide if match is strong enough.
# - If below threshold, suggests the closest related question and its answer as fallback.
# - This approach handles paraphrased or partially matching queries effectively, improving answer relevance.
#
# Multilingual support, named entity preservation, and input validation increase usability and relevance.
# Logging and modular design support ongoing improvement and easy deployment.

# 📦 Libraries Installed and Their Purpose
# langdetect            → Language detection for multilingual input text
# deep_translator       → GoogleTranslator API wrapper for language translation
# sentence_transformers → Semantic similarity model & utilities for embedding & cosine similarity
# torch                 → PyTorch for tensor computations and GPU acceleration
# pandas                → Data handling and manipulation for FAQ dataset
# gradio                → Web UI for chatbot interface
# re                    → Regular expressions for text cleaning and question splitting
# transformers          → Hugging Face pipelines for question answering and GPT text generation
# fuzzywuzzy            → Fuzzy string matching for approximate text matching
# logging               → Logging user queries and interactions to file

# ⚙️ Device Setup
# torch.cuda.is_available() → Checks if GPU is available for faster model inference
# device = 'cuda' or 'cpu'  → Sets device string for model/pipeline placement

# 🗂️ GUVI Keywords List
# A comprehensive list of brand names, course names, synonyms, and common typos
# Used for relevance checking of user queries against GUVI topics

# 📥 Load FAQ Dataset
# pd.read_csv(...)         → Load FAQ questions and answers from a Google Sheets CSV export
# df.dropna()              → Remove empty rows
# df.drop_duplicates(...)  → Remove duplicates ignoring case & extra spaces

# 💡 Semantic Search Model & Embeddings
# SentenceTransformer(...) → Load pretrained MiniLM model for embeddings
# model.encode(questions) → Compute vector embeddings for semantic similarity search

# 🧭 Named Entity Mapping
# entity_map dictionary    → Maps local language named entities to English/GUVI official names
# preserve_named_entities(text, lang) → Replaces native names with English equivalents before translation

# 🔍 Semantic Search Function
# model.encode(query)          → Embed the query into vector space
# util.pytorch_cos_sim(...)    → Compute cosine similarity between query and FAQ embeddings
# torch.max(...) and torch.argmax(...) → Find best matching FAQ
# Returns FAQ answer if above threshold else suggested similar question + answer

# 🔁 RAG QA Fallback
# rag_model.encode(corpus)     → Embed fallback corpus documents
# faiss.IndexFlatL2(...)       → Build FAISS index for efficient similarity search
# index.search(query_vector)   → Retrieve top relevant corpus passages
# pipeline("question-answering") → Run QA model to extract answer from passages
# Returns answer if confident, else fallback message

# 🤖 GPT Fallback (FLAN-T5)
# pipeline("text2text-generation") → Load GPT-based text generation model
# gpt_generator(prompt, ...)        → Generate natural language answer from controlled prompt with GUVI context
# Error handling included to avoid crashes

# ✂️ Split Multiple Questions
# re.split(...)                 → Split user input on delimiters (?, &, "and", newline)
# Returns list of individual questions

# 📄 Logging
# logging.basicConfig(...)      → Configure file logging format, file location, and level
# logging.info(...)             → Write user question, category, fallback method to log file

# 🧠 Multilingual Chatbot Logic
# detect(user_input)            → Detect language of full input or per question
# preserve_named_entities(...)  → Replace named entities before translation
# GoogleTranslator(...)         → Translate user question to English & final answer back to user's language
# is_guvi_related(...)          → Check if question is about GUVI using keyword list
# find_best_answer(...)         → Semantic search over FAQ
# rag_fallback(...)             → Retrieve answer from fallback RAG corpus
# gpt_fallback(...)             → Generate answer via GPT fallback if needed
# log_interaction(...)          → Log the interaction details
# Returns combined answers to user input

# 🌐 Gradio UI
# gr.Interface(...)            → Define web app UI with input textbox and output text area
# demo.launch(...)             → Run the app locally or share via public link

# Error Handling Sections
# try-except blocks used extensively for:
# - Loading models and data (semantic search, RAG, GPT)
# - Translation API calls
# - Semantic search and fallback queries
# - Logging and user input validation

# | Area                     | What to Do                                   | Where/Function                                      |
# | ------------------------ | -------------------------------------------- | --------------------------------------------------- |
# | Entity Map               | Add new mappings, extend language support    | entity_map, preserve_named_entities()               |
# | Input Validation         | Add new checks (Tanglish, short phrases)     | is_valid_input()                                    |
# | Semantic Search          | Adjust threshold, add fuzzy matching?        | find_best_answer()                                  |
# | Fallback Logic           | Modify chaining, update prompt or thresholds | multilingual_qa_bot(),rag_fallback(, gpt_fallback() |
# | Logging                  | Log more metadata, fallback types            | log_interaction()`                                  |
# | Translation Steps        | Preserve entities pre/post translation       | preserve_named_entities(), translation calls        |
# | Gradio UI                | Update placeholders, instructions            | Gradio interface setup                              |
# | Error Handling           | Update messages, extend try-except blocks    | Throughout main functions                           |
# | Documentation & Comments | Add descriptions of new logic                | Top comments, function docstrings                   |
# | Testing & Quality        | Add tests, verify new logic                  | Test scripts / manual testing                       |


# Overall, this project integrates:
# - Multilingual input support & translation
# - Semantic FAQ search with approximate matching
# - Retrieval Augmented Generation fallback with QA model
# - GPT-based text generation fallback for open-ended queries
# - Robust input validation & error handling
# - User interaction logging for analytics
# - Easy-to-use web UI with Gradio

"""### Log File Management: Checking & Downloading Logs"""

# 📂 Chatbot Interaction Logging Setup

import logging                              # 📦 Import Python logging module for logging events
import os                                   # 📦 Import os module to interact with operating system

# 🔄 Remove all existing logging handlers to reset logging configuration (needed in notebooks)
for handler in logging.root.handlers[:]:    # 🔁 Loop through all current logging handlers
    logging.root.removeHandler(handler)     # ❌ Remove each handler to allow reconfiguration

# ⚙️ Configure logging settings to write logs to a file
logging.basicConfig(
    filename='chatbot_interactions.log',                 # 📝 Log file name where logs will be saved
    filemode='a',                                        # ➕ Append mode to add new logs without overwriting existing
    format='%(asctime)s | %(levelname)s | %(message)s',  # 📝 Log message format: timestamp, level, message
    level=logging.INFO                                   # ℹ️ Minimum log level to capture INFO and above
)

# 📝 Write a test log entry to verify logging setup
logging.info("Test log entry: chatbot started")          # ℹ️ Log an informational message

# 🔍 Check if the log file was created successfully and print result
print("Log file exists?", os.path.exists('chatbot_interactions.log'))  # Prints True if file exists

# Logging Setup for Chatbot Interactions

# Short Description:
# Resets existing logging handlers and configures Python logging to write INFO level and
#  above messages into a file named chatbot_interactions.log.
# It appends new logs without overwriting old ones, using a clear timestamped format.
# A test log entry is written, and the presence of the log file is confirmed.

# Key Features:
# - Removes previous logging handlers to enable reconfiguration in notebook environments.
# - Uses append mode (`filemode='a'`) to preserve log history.
# - Logs messages with timestamp, level, and text for easy tracking.
# - Confirms log file creation to ensure logging is working.

# Logic:
# 1. Clear existing logging handlers to avoid conflicts.
# 2. Set up new logging configuration with file output and formatting.
# 3. Write a test log message to validate the setup.
# 4. Check and print whether the log file exists in the current directory.

# 📋 List Log File Information
!ls -l chatbot_interactions.log    # List detailed info (permissions, size, timestamp) of the log file

# ⬇️ Download Chatbot Interactions Log
from google.colab import files
files.download('chatbot_interactions.log')   # Download the chatbot log file to your local machine

"""## Semantic Accuracy Evaluation

"""

# Library Installation & Setup

# Install Natural Language Toolkit
!pip install nltk           # Install the NLTK library for natural language processing

# Install SentencePiece tokenizer
!pip install sentencepiece  # Install the SentencePiece library for tokenization

# 📦 Libraries Installed and Their Purpose
# 🧠 nltk           → Natural Language Toolkit for various NLP tasks like tokenization, parsing, classification
# 🧩 sentencepiece  → Language-independent tokenizer and detokenizer used for subword tokenization

# 📦 Import NLTK and Download Tokenizer Model

import nltk                         # Import the Natural Language Toolkit for NLP tasks
nltk.download('punkt')              # Download the 'punkt' tokenizer model for sentence and word tokenization

# 📄 Short Description:
# 'punkt' is a pre-trained tokenizer model used by NLTK
# It splits text into sentences and words, handling punctuation effectively
# Useful for preparing raw text for NLP processing and semantic analysis

# 📋 Semantic Classification Report – PRECISION, RECALL, F1

# 📦 Import necessary libraries
from sentence_transformers import SentenceTransformer, util  # Sentence embedding and similarity utilities
from deep_translator import GoogleTranslator                 # Language translation for multilingual normalization
from sklearn.metrics import classification_report            # Classification report for evaluation metrics
import string                                                # For punctuation removal in normalization

# 🔍 Load Pretrained Sentence Transformer Model
model = SentenceTransformer('all-MiniLM-L6-v2')              # Load MiniLM model for sentence embeddings

# 📚 Define Ground Truth (Expected) and Model Output (Predicted) Answers
expected_answers = [                                          # List of expected correct answers
    "Yes, GUVI is a well-recognized platform partnered with IIT-M & offers quality tech courses.",
    "GUVI offers job assistance through placement support and career guidance.",
    "Yes, learners find GUVI highly worth it due to mentorship, certification & outcomes.",
    "Yes, GUVI certificates are widely recognized and boost job chances.",
    "Yes, GUVI is secure and backed by trusted institutions like IIT-M & NASSCOM.",
    "The brand ambassador of GUVI is Smriti Mandhana."
]

predicted_answers = [                                         # List of predicted answers from the model
    "Yes, GUVI is a well-recognized platform partnered with IIT-M & offers quality tech courses.",
    "GUVI offers assistance at work through placement and professional guidance support.",
    "Yes, learners find GUVI highly worth it due to mentorship, certification & outcomes.",
    "Yes, GUVI certificates are widely recognized and boost job chances.",
    "Yes, GUVI is secure and backed by trusted institutions like IIT-M & NASSCOM.",
    "Cricketer smriti mandhana"
]

# 🌐 Translation helper for multilingual support
def translate_to_en(text):                                   # Translate input text to English
    try:
        return GoogleTranslator(source='auto', target='en').translate(text)  # Attempt translation
    except:
        return text                                           # Return original if translation fails

# 🧹 Normalize text function (lowercase + translation + punctuation removal + phrase fix)
def normalize_text(text):
    translated = translate_to_en(text).strip().lower()                            # Translate and lowercase
    translated = translated.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    # Fix known phrase for consistency
    if "smriti" in translated and "mandhana" in translated:
        return "the brand ambassador of guvi is smriti mandhana"
    return translated

# 🔄 Normalize all expected and predicted answers for semantic matching
normalized_expected = [normalize_text(ans) for ans in expected_answers]            # Normalize expected answers
normalized_predicted = [normalize_text(ans) for ans in predicted_answers]          # Normalize predicted answers

# 🧠 Encode sentences in batch for efficiency
expected_embeddings = model.encode(normalized_expected, convert_to_tensor=True)    # Encode expected answers
predicted_embeddings = model.encode(normalized_predicted, convert_to_tensor=True)  # Encode predicted answers

# 📏 Calculate cosine similarity scores between embeddings
similarity_scores = util.cos_sim(expected_embeddings, predicted_embeddings).diagonal()  # Pairwise diagonal similarity

# ⚖️ Define threshold for semantic match (above which answers considered matching)
threshold = 0.85

# 🏷️ Generate predicted binary labels based on similarity threshold
y_true = [1] * len(expected_answers)                                      # True labels (all matches expected)
y_pred = [1 if score >= threshold else 0 for score in similarity_scores]  # Predicted matches

# 🖨️ Print detailed evaluation report
print("📊 Semantic Evaluation Report:\n")
for i, (exp, pred, score, label) in enumerate(zip(normalized_expected, normalized_predicted, similarity_scores, y_pred), start=1):
    print(f"🟡 Q{i}")
    print(f"✅ Expected: {exp}")
    print(f"🤖 Predicted: {pred}")
    print(f"🔗 Similarity Score: {score:.4f}")
    print("✅ Match\n" if label == 1 else "❌ No Match\n")

# 📈 Calculate and print final accuracy score
accuracy = sum(y_pred) / len(y_pred)                           # Calculate proportion of correctly matched predictions
print(f"📈 Final Semantic Accuracy: {accuracy * 100:.2f}%\n")  # Print accuracy as percentage with 2 decimal places

# 📋 Generate classification report with precision, recall, F1-score
print("📋 Classification Report:")           # Header print statement for clarity
print(classification_report(                  # Generate and print detailed classification metrics
    y_true,                                   # True binary labels (ground truth)
    y_pred,                                   # Predicted binary labels from similarity thresholding
    labels=[0, 1],                            # Define label order: 0 = No Match, 1 = Match
    target_names=["No Match", "Match"],       # Names corresponding to label classes
    zero_division=0                           # Avoid division by zero warnings for classes with no samples
))

# 📄 Short Description:
# This code evaluates semantic similarity between expected and predicted answers
# using sentence embeddings and cosine similarity, to determine if model outputs
# semantically match the ground truth. It computes accuracy and provides a detailed
# classification report including precision, recall, and F1-score metrics.

# 🌟 Key Features:
# - Semantic similarity calculation using sentence embeddings
# - Multilingual text normalization via translation and punctuation removal
# - Threshold-based classification of matches/non-matches
# - Comprehensive evaluation metrics via classification report
# - Handles minor phrase variations and typos with semantic matching

# 🎯 Threshold Value & Explanation:
# Threshold = 0.85
# Cosine similarity scores above this threshold are considered semantic matches,
# indicating predicted answers are sufficiently close in meaning to expected answers.

# 📋 Classification Report Overview:
# Provides precision, recall, and F1-score for:
# - Match class: correctly identified semantically matching answers
# - No Match class: correctly identified non-matching answers

# 📈 Metrics Explained:
# Precision  → How many predicted matches are actually correct
# Recall     → How many actual matches were correctly predicted
# F1-score   → Harmonic mean of precision and recall, overall accuracy measure
# Accuracy   → Overall fraction of correct semantic match predictions

# 🧠 Model Used:
# SentenceTransformer 'all-MiniLM-L6-v2' — a lightweight, efficient model for semantic sentence embeddings.

# 🛠️ Tools Used:
# - sentence_transformers for embedding and cosine similarity
# - deep_translator for automatic text translation to English
# - sklearn.metrics for classification report
# - Python standard libraries (string, print, etc.)

# 🔍 Logic Summary:
# 1. Normalize text by translating to English and removing punctuation.
# 2. Encode expected and predicted answers to embeddings using SentenceTransformer.
# 3. Compute cosine similarity between corresponding expected and predicted pairs.
# 4. Use threshold to assign match/non-match labels.
# 5. Calculate accuracy and generate classification report to evaluate model performance.

# 📦 Commands Used (Category-wise with Description):

# Text Normalization & Translation -
# GoogleTranslator(source='auto', target='en').translate(text)
#   → Translates text from detected source language to English for uniform comparison.
# str.translate(str.maketrans('', '', string.punctuation))
#   → Removes punctuation from translated text for clean matching.

# Sentence Embedding & Similarity
# model.encode(text_list, convert_to_tensor=True)
#   → Converts list of sentences into embedding tensors using SentenceTransformer.
# util.cos_sim(embedding1, embedding2).diagonal()
#   → Computes cosine similarity scores for corresponding sentence pairs.

# Thresholding & Label Generation
# [1 if score >= threshold else 0 for score in similarity_scores]
#   → Converts similarity scores into binary match (1) or no-match (0) labels.

# Accuracy & Reporting
# sum(y_pred) / len(y_pred)
#   → Calculates overall semantic match accuracy.
# classification_report(y_true, y_pred, labels=[0,1], target_names=["No Match","Match"], zero_division=0)
#   → Generates detailed classification report with precision, recall, F1 metrics per class.
# print(...)
#   → Outputs accuracy and classification results for user review.